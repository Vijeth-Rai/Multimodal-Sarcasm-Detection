{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1facc9d",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition Model\n",
    "\n",
    "This model and weights are from https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch The github project is cloned and modified to fit the required needs.\n",
    "\n",
    "The images in final_image_folder folder, which contains faces of all the speakers through use of MTCNN. All the images in this folder are sent through the model to generate embeddings. For each face detected in the image, each of the faces generate embeddings through FER model. All the embeddings are then averaged unless there is null or 0 embeddings. The logic behind averaging is that, everyone in the image/room would have same or similar facial expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8b78cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/645 [00:00<?, ?it/s]C:\\Users\\vijet\\Projects\\MSc\\mustard++\\Facial-Expression-Recognition.Pytorch-master\\transforms\\functional.py:63: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [07:42<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "import pandas as pd\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "cut_size = 44\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.TenCrop(cut_size),\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "])\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "class ModifiedVGG(nn.Module):\n",
    "    def __init__(self, vgg_name, embedding_dim=256):\n",
    "        super(ModifiedVGG, self).__init__()\n",
    "        \n",
    "        # Original VGG features\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        \n",
    "        # Embedding layer to get the embeddings from the model\n",
    "        self.embedding_layer = nn.Linear(512, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Pass through the embedding layer\n",
    "        embedding = self.embedding_layer(out)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Load pre-trained weights into modified VGG\n",
    "transfer_net = ModifiedVGG('VGG19')\n",
    "checkpoint = torch.load(os.path.join('FER2013_VGG19', 'PrivateTest_model.t7'))\n",
    "\n",
    "# Remove the 'classifier' weights from the checkpoint as they don't exist in the ModifiedVGG\n",
    "checkpoint['net'] = {k: v for k, v in checkpoint['net'].items() if 'classifier' not in k}\n",
    "transfer_net.load_state_dict(checkpoint['net'], strict=False)  # Use strict=False since the model architectures differ slightly\n",
    "\n",
    "transfer_net.cuda()\n",
    "transfer_net.eval()\n",
    "\n",
    "# Now you can pass your images through `transfer_net` to get the embeddings\n",
    "# Prepare to collect embeddings and filenames\n",
    "averaged_embeddings_first_half = []\n",
    "image_names_first_half = []\n",
    "\n",
    "image_names = os.listdir('final_images_faces/')\n",
    "\n",
    "for img_name in tqdm(image_names[:645]):\n",
    "    img_path = os.path.join('final_images_faces/', img_name)\n",
    "    \n",
    "    raw_img = io.imread(img_path)\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = mtcnn.detect_faces(raw_img)\n",
    "    \n",
    "    img_embeddings = []  # To collect embeddings for this image\n",
    "    \n",
    "    for face in faces:\n",
    "        cropped_face = face['box']  # This gives you the bounding box of the face\n",
    "        \n",
    "        gray = rgb2gray(raw_img)\n",
    "        gray = resize(gray, (48, 48), mode='symmetric').astype(np.uint8)\n",
    "    \n",
    "        img = gray[:, :, np.newaxis]\n",
    "        img = np.concatenate((img, img, img), axis=2)\n",
    "        img = Image.fromarray(img)\n",
    "        inputs = transform_test(img)\n",
    "    \n",
    "        ncrops, c, h, w = np.shape(inputs)\n",
    "        inputs = inputs.view(-1, c, h, w)\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "        # Note: 'volatile' is deprecated. Instead, use 'with torch.no_grad():' for inference\n",
    "        with torch.no_grad():\n",
    "            embed = transfer_net(inputs)\n",
    "        \n",
    "        # Check validity of embedding\n",
    "        if torch.any(embed):  # If any value in the embedding tensor is non-zero\n",
    "            img_embeddings.append(embed.cpu().numpy())\n",
    "\n",
    "        \n",
    "    # Average the embeddings for this image\n",
    "    if img_embeddings:  # If there's at least one valid embedding\n",
    "        avg_embed = np.mean(img_embeddings, axis=0)\n",
    "    else:\n",
    "        avg_embed = np.zeros((256,))  # Placeholder for no embedding\n",
    "    \n",
    "    # Add averaged embedding and related info to lists\n",
    "    averaged_embeddings_first_half.append(avg_embed)\n",
    "    base_name = os.path.splitext(img_name)[0]\n",
    "    image_names_first_half.append(base_name)\n",
    "\n",
    "\n",
    "# Create a DataFrame with averaged embeddings and related info for the first half\n",
    "df_avg_embeddings_first_half = pd.DataFrame({\n",
    "    'ImageName': image_names_first_half,\n",
    "    'Embedding': averaged_embeddings_first_half\n",
    "})\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Averaging each list of embeddings\n",
    "averaged_embeddings = [np.mean(embedding, axis=0) for embedding in df_avg_embeddings_first_half['Embedding']]\n",
    "\n",
    "# Filter the rows where the 'Embedding' column is not an instance of numpy.ndarray\n",
    "df_cleaned = df_avg_embeddings_first_half[df_avg_embeddings_first_half['Embedding'].apply(lambda x: isinstance(x, np.ndarray))]\n",
    "\n",
    "# If you want to reset the index after dropping:\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert the 'Embedding' column from a nested list to a numpy array\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(np.array)\n",
    "\n",
    "# Average the embeddings along axis=1\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(lambda x: x.mean(axis=0) if len(x.shape) > 1 else x)\n",
    "\n",
    "# Expand the averaged embeddings into individual columns\n",
    "embeddings_df = df_cleaned['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "embeddings_df.columns = [f'embed_{i}' for i in range(embeddings_df.shape[1])]\n",
    "\n",
    "# Drop the original 'Embedding' column and concatenate the expanded columns\n",
    "df_avg_embeddings_first_half = pd.concat([df_cleaned.drop('Embedding', axis=1), embeddings_df], axis=1)\n",
    "\n",
    "\n",
    "df_avg_embeddings_first_half.to_csv('645_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a0983b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/645 [00:00<?, ?it/s]C:\\Users\\vijet\\Projects\\MSc\\mustard++\\Facial-Expression-Recognition.Pytorch-master\\transforms\\functional.py:63: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [11:19<00:00,  1.05s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "import pandas as pd\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "cut_size = 44\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.TenCrop(cut_size),\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "])\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "class ModifiedVGG(nn.Module):\n",
    "    def __init__(self, vgg_name, embedding_dim=256):\n",
    "        super(ModifiedVGG, self).__init__()\n",
    "        \n",
    "        # Original VGG features\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        \n",
    "        # Embedding layer to get the embeddings from the model\n",
    "        self.embedding_layer = nn.Linear(512, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Pass through the embedding layer\n",
    "        embedding = self.embedding_layer(out)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Load pre-trained weights into modified VGG\n",
    "transfer_net = ModifiedVGG('VGG19')\n",
    "checkpoint = torch.load(os.path.join('FER2013_VGG19', 'PrivateTest_model.t7'))\n",
    "\n",
    "# Remove the 'classifier' weights from the checkpoint as they don't exist in the ModifiedVGG\n",
    "checkpoint['net'] = {k: v for k, v in checkpoint['net'].items() if 'classifier' not in k}\n",
    "transfer_net.load_state_dict(checkpoint['net'], strict=False)  # Use strict=False since the model architectures differ slightly\n",
    "\n",
    "transfer_net.cuda()\n",
    "transfer_net.eval()\n",
    "\n",
    "# Now you can pass your images through `transfer_net` to get the embeddings\n",
    "# Prepare to collect embeddings and filenames\n",
    "averaged_embeddings_first_half = []\n",
    "image_names_first_half = []\n",
    "\n",
    "image_names = os.listdir('final_images_faces/')\n",
    "\n",
    "for img_name in tqdm(image_names[645:1290]):\n",
    "    img_path = os.path.join('final_images_faces/', img_name)\n",
    "    \n",
    "    raw_img = io.imread(img_path)\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = mtcnn.detect_faces(raw_img)\n",
    "    \n",
    "    img_embeddings = []  # To collect embeddings for this image\n",
    "    \n",
    "    for face in faces:\n",
    "        cropped_face = face['box']  # This gives you the bounding box of the face\n",
    "        \n",
    "        gray = rgb2gray(raw_img)\n",
    "        gray = resize(gray, (48, 48), mode='symmetric').astype(np.uint8)\n",
    "    \n",
    "        img = gray[:, :, np.newaxis]\n",
    "        img = np.concatenate((img, img, img), axis=2)\n",
    "        img = Image.fromarray(img)\n",
    "        inputs = transform_test(img)\n",
    "    \n",
    "        ncrops, c, h, w = np.shape(inputs)\n",
    "        inputs = inputs.view(-1, c, h, w)\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "        # Note: 'volatile' is deprecated. Instead, use 'with torch.no_grad():' for inference\n",
    "        with torch.no_grad():\n",
    "            embed = transfer_net(inputs)\n",
    "        \n",
    "        # Check validity of embedding\n",
    "        if torch.any(embed):  # If any value in the embedding tensor is non-zero\n",
    "            img_embeddings.append(embed.cpu().numpy())\n",
    "\n",
    "        \n",
    "    # Average the embeddings for this image\n",
    "    if img_embeddings:  # If there's at least one valid embedding\n",
    "        avg_embed = np.mean(img_embeddings, axis=0)\n",
    "    else:\n",
    "        avg_embed = np.zeros((256,))  # Placeholder for no embedding\n",
    "    \n",
    "    # Add averaged embedding and related info to lists\n",
    "    averaged_embeddings_first_half.append(avg_embed)\n",
    "    base_name = os.path.splitext(img_name)[0]\n",
    "    image_names_first_half.append(base_name)\n",
    "\n",
    "\n",
    "# Create a DataFrame with averaged embeddings and related info for the first half\n",
    "df_avg_embeddings_first_half = pd.DataFrame({\n",
    "    'ImageName': image_names_first_half,\n",
    "    'Embedding': averaged_embeddings_first_half\n",
    "})\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Averaging each list of embeddings\n",
    "averaged_embeddings = [np.mean(embedding, axis=0) for embedding in df_avg_embeddings_first_half['Embedding']]\n",
    "\n",
    "# Filter the rows where the 'Embedding' column is not an instance of numpy.ndarray\n",
    "df_cleaned = df_avg_embeddings_first_half[df_avg_embeddings_first_half['Embedding'].apply(lambda x: isinstance(x, np.ndarray))]\n",
    "\n",
    "# If you want to reset the index after dropping:\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert the 'Embedding' column from a nested list to a numpy array\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(np.array)\n",
    "\n",
    "# Average the embeddings along axis=1\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(lambda x: x.mean(axis=0) if len(x.shape) > 1 else x)\n",
    "\n",
    "# Expand the averaged embeddings into individual columns\n",
    "embeddings_df = df_cleaned['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "embeddings_df.columns = [f'embed_{i}' for i in range(embeddings_df.shape[1])]\n",
    "\n",
    "# Drop the original 'Embedding' column and concatenate the expanded columns\n",
    "df_avg_embeddings_first_half = pd.concat([df_cleaned.drop('Embedding', axis=1), embeddings_df], axis=1)\n",
    "\n",
    "\n",
    "df_avg_embeddings_first_half.to_csv('645_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d7f8d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/645 [00:00<?, ?it/s]C:\\Users\\vijet\\Projects\\MSc\\mustard++\\Facial-Expression-Recognition.Pytorch-master\\transforms\\functional.py:63: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [11:33<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "import pandas as pd\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "cut_size = 44\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.TenCrop(cut_size),\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "])\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "class ModifiedVGG(nn.Module):\n",
    "    def __init__(self, vgg_name, embedding_dim=256):\n",
    "        super(ModifiedVGG, self).__init__()\n",
    "        \n",
    "        # Original VGG features\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        \n",
    "        # Embedding layer to get the embeddings from the model\n",
    "        self.embedding_layer = nn.Linear(512, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Pass through the embedding layer\n",
    "        embedding = self.embedding_layer(out)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Load pre-trained weights into modified VGG\n",
    "transfer_net = ModifiedVGG('VGG19')\n",
    "checkpoint = torch.load(os.path.join('FER2013_VGG19', 'PrivateTest_model.t7'))\n",
    "\n",
    "# Remove the 'classifier' weights from the checkpoint as they don't exist in the ModifiedVGG\n",
    "checkpoint['net'] = {k: v for k, v in checkpoint['net'].items() if 'classifier' not in k}\n",
    "transfer_net.load_state_dict(checkpoint['net'], strict=False)  # Use strict=False since the model architectures differ slightly\n",
    "\n",
    "transfer_net.cuda()\n",
    "transfer_net.eval()\n",
    "\n",
    "# Now you can pass your images through `transfer_net` to get the embeddings\n",
    "# Prepare to collect embeddings and filenames\n",
    "averaged_embeddings_first_half = []\n",
    "image_names_first_half = []\n",
    "\n",
    "image_names = os.listdir('final_images_faces/')\n",
    "\n",
    "for img_name in tqdm(image_names[1290:1935]):\n",
    "    img_path = os.path.join('final_images_faces/', img_name)\n",
    "    \n",
    "    raw_img = io.imread(img_path)\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = mtcnn.detect_faces(raw_img)\n",
    "    \n",
    "    img_embeddings = []  # To collect embeddings for this image\n",
    "    \n",
    "    for face in faces:\n",
    "        cropped_face = face['box']  # This gives you the bounding box of the face\n",
    "        \n",
    "        gray = rgb2gray(raw_img)\n",
    "        gray = resize(gray, (48, 48), mode='symmetric').astype(np.uint8)\n",
    "    \n",
    "        img = gray[:, :, np.newaxis]\n",
    "        img = np.concatenate((img, img, img), axis=2)\n",
    "        img = Image.fromarray(img)\n",
    "        inputs = transform_test(img)\n",
    "    \n",
    "        ncrops, c, h, w = np.shape(inputs)\n",
    "        inputs = inputs.view(-1, c, h, w)\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "        # Note: 'volatile' is deprecated. Instead, use 'with torch.no_grad():' for inference\n",
    "        with torch.no_grad():\n",
    "            embed = transfer_net(inputs)\n",
    "        \n",
    "        # Check validity of embedding\n",
    "        if torch.any(embed):  # If any value in the embedding tensor is non-zero\n",
    "            img_embeddings.append(embed.cpu().numpy())\n",
    "\n",
    "        \n",
    "    # Average the embeddings for this image\n",
    "    if img_embeddings:  # If there's at least one valid embedding\n",
    "        avg_embed = np.mean(img_embeddings, axis=0)\n",
    "    else:\n",
    "        avg_embed = np.zeros((256,))  # Placeholder for no embedding\n",
    "    \n",
    "    # Add averaged embedding and related info to lists\n",
    "    averaged_embeddings_first_half.append(avg_embed)\n",
    "    base_name = os.path.splitext(img_name)[0]\n",
    "    image_names_first_half.append(base_name)\n",
    "\n",
    "\n",
    "# Create a DataFrame with averaged embeddings and related info for the first half\n",
    "df_avg_embeddings_first_half = pd.DataFrame({\n",
    "    'ImageName': image_names_first_half,\n",
    "    'Embedding': averaged_embeddings_first_half\n",
    "})\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Averaging each list of embeddings\n",
    "averaged_embeddings = [np.mean(embedding, axis=0) for embedding in df_avg_embeddings_first_half['Embedding']]\n",
    "\n",
    "# Filter the rows where the 'Embedding' column is not an instance of numpy.ndarray\n",
    "df_cleaned = df_avg_embeddings_first_half[df_avg_embeddings_first_half['Embedding'].apply(lambda x: isinstance(x, np.ndarray))]\n",
    "\n",
    "# If you want to reset the index after dropping:\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert the 'Embedding' column from a nested list to a numpy array\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(np.array)\n",
    "\n",
    "# Average the embeddings along axis=1\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(lambda x: x.mean(axis=0) if len(x.shape) > 1 else x)\n",
    "\n",
    "# Expand the averaged embeddings into individual columns\n",
    "embeddings_df = df_cleaned['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "embeddings_df.columns = [f'embed_{i}' for i in range(embeddings_df.shape[1])]\n",
    "\n",
    "# Drop the original 'Embedding' column and concatenate the expanded columns\n",
    "df_avg_embeddings_first_half = pd.concat([df_cleaned.drop('Embedding', axis=1), embeddings_df], axis=1)\n",
    "\n",
    "\n",
    "df_avg_embeddings_first_half.to_csv('645_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4973bbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/645 [00:00<?, ?it/s]C:\\Users\\vijet\\Projects\\MSc\\mustard++\\Facial-Expression-Recognition.Pytorch-master\\transforms\\functional.py:63: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 645/645 [10:25<00:00,  1.03it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "import pandas as pd\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "cut_size = 44\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.TenCrop(cut_size),\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "])\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "class ModifiedVGG(nn.Module):\n",
    "    def __init__(self, vgg_name, embedding_dim=256):\n",
    "        super(ModifiedVGG, self).__init__()\n",
    "        \n",
    "        # Original VGG features\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        \n",
    "        # Embedding layer to get the embeddings from the model\n",
    "        self.embedding_layer = nn.Linear(512, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Pass through the embedding layer\n",
    "        embedding = self.embedding_layer(out)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Load pre-trained weights into modified VGG\n",
    "transfer_net = ModifiedVGG('VGG19')\n",
    "checkpoint = torch.load(os.path.join('FER2013_VGG19', 'PrivateTest_model.t7'))\n",
    "\n",
    "# Remove the 'classifier' weights from the checkpoint as they don't exist in the ModifiedVGG\n",
    "checkpoint['net'] = {k: v for k, v in checkpoint['net'].items() if 'classifier' not in k}\n",
    "transfer_net.load_state_dict(checkpoint['net'], strict=False)  # Use strict=False since the model architectures differ slightly\n",
    "\n",
    "transfer_net.cuda()\n",
    "transfer_net.eval()\n",
    "\n",
    "# Now you can pass your images through `transfer_net` to get the embeddings\n",
    "# Prepare to collect embeddings and filenames\n",
    "averaged_embeddings_first_half = []\n",
    "image_names_first_half = []\n",
    "\n",
    "image_names = os.listdir('final_images_faces/')\n",
    "\n",
    "for img_name in tqdm(image_names[1935:2580]):\n",
    "    img_path = os.path.join('final_images_faces/', img_name)\n",
    "    \n",
    "    raw_img = io.imread(img_path)\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = mtcnn.detect_faces(raw_img)\n",
    "    \n",
    "    img_embeddings = []  # To collect embeddings for this image\n",
    "    \n",
    "    for face in faces:\n",
    "        cropped_face = face['box']  # This gives you the bounding box of the face\n",
    "        \n",
    "        gray = rgb2gray(raw_img)\n",
    "        gray = resize(gray, (48, 48), mode='symmetric').astype(np.uint8)\n",
    "    \n",
    "        img = gray[:, :, np.newaxis]\n",
    "        img = np.concatenate((img, img, img), axis=2)\n",
    "        img = Image.fromarray(img)\n",
    "        inputs = transform_test(img)\n",
    "    \n",
    "        ncrops, c, h, w = np.shape(inputs)\n",
    "        inputs = inputs.view(-1, c, h, w)\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "        # Note: 'volatile' is deprecated. Instead, use 'with torch.no_grad():' for inference\n",
    "        with torch.no_grad():\n",
    "            embed = transfer_net(inputs)\n",
    "        \n",
    "        # Check validity of embedding\n",
    "        if torch.any(embed):  # If any value in the embedding tensor is non-zero\n",
    "            img_embeddings.append(embed.cpu().numpy())\n",
    "\n",
    "        \n",
    "    # Average the embeddings for this image\n",
    "    if img_embeddings:  # If there's at least one valid embedding\n",
    "        avg_embed = np.mean(img_embeddings, axis=0)\n",
    "    else:\n",
    "        avg_embed = np.zeros((256,))  # Placeholder for no embedding\n",
    "    \n",
    "    # Add averaged embedding and related info to lists\n",
    "    averaged_embeddings_first_half.append(avg_embed)\n",
    "    base_name = os.path.splitext(img_name)[0]\n",
    "    image_names_first_half.append(base_name)\n",
    "\n",
    "\n",
    "# Create a DataFrame with averaged embeddings and related info for the first half\n",
    "df_avg_embeddings_first_half = pd.DataFrame({\n",
    "    'ImageName': image_names_first_half,\n",
    "    'Embedding': averaged_embeddings_first_half\n",
    "})\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Averaging each list of embeddings\n",
    "averaged_embeddings = [np.mean(embedding, axis=0) for embedding in df_avg_embeddings_first_half['Embedding']]\n",
    "\n",
    "# Filter the rows where the 'Embedding' column is not an instance of numpy.ndarray\n",
    "df_cleaned = df_avg_embeddings_first_half[df_avg_embeddings_first_half['Embedding'].apply(lambda x: isinstance(x, np.ndarray))]\n",
    "\n",
    "# If you want to reset the index after dropping:\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert the 'Embedding' column from a nested list to a numpy array\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(np.array)\n",
    "\n",
    "# Average the embeddings along axis=1\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(lambda x: x.mean(axis=0) if len(x.shape) > 1 else x)\n",
    "\n",
    "# Expand the averaged embeddings into individual columns\n",
    "embeddings_df = df_cleaned['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "embeddings_df.columns = [f'embed_{i}' for i in range(embeddings_df.shape[1])]\n",
    "\n",
    "# Drop the original 'Embedding' column and concatenate the expanded columns\n",
    "df_avg_embeddings_first_half = pd.concat([df_cleaned.drop('Embedding', axis=1), embeddings_df], axis=1)\n",
    "\n",
    "\n",
    "df_avg_embeddings_first_half.to_csv('645_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e148a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/465 [00:00<?, ?it/s]C:\\Users\\vijet\\Projects\\MSc\\mustard++\\Facial-Expression-Recognition.Pytorch-master\\transforms\\functional.py:63: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 465/465 [07:23<00:00,  1.05it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "import pandas as pd\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "cut_size = 44\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.TenCrop(cut_size),\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "])\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "class ModifiedVGG(nn.Module):\n",
    "    def __init__(self, vgg_name, embedding_dim=256):\n",
    "        super(ModifiedVGG, self).__init__()\n",
    "        \n",
    "        # Original VGG features\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        \n",
    "        # Embedding layer to get the embeddings from the model\n",
    "        self.embedding_layer = nn.Linear(512, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Pass through the embedding layer\n",
    "        embedding = self.embedding_layer(out)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Load pre-trained weights into modified VGG\n",
    "transfer_net = ModifiedVGG('VGG19')\n",
    "checkpoint = torch.load(os.path.join('FER2013_VGG19', 'PrivateTest_model.t7'))\n",
    "\n",
    "# Remove the 'classifier' weights from the checkpoint as they don't exist in the ModifiedVGG\n",
    "checkpoint['net'] = {k: v for k, v in checkpoint['net'].items() if 'classifier' not in k}\n",
    "transfer_net.load_state_dict(checkpoint['net'], strict=False)  # Use strict=False since the model architectures differ slightly\n",
    "\n",
    "transfer_net.cuda()\n",
    "transfer_net.eval()\n",
    "\n",
    "# Now you can pass your images through `transfer_net` to get the embeddings\n",
    "# Prepare to collect embeddings and filenames\n",
    "averaged_embeddings_first_half = []\n",
    "image_names_first_half = []\n",
    "\n",
    "image_names = os.listdir('final_images_faces/')\n",
    "\n",
    "for img_name in tqdm(image_names[2580:]):\n",
    "    img_path = os.path.join('final_images_faces/', img_name)\n",
    "    \n",
    "    raw_img = io.imread(img_path)\n",
    "    \n",
    "    # Detect faces in the image\n",
    "    faces = mtcnn.detect_faces(raw_img)\n",
    "    \n",
    "    img_embeddings = []  # To collect embeddings for this image\n",
    "    \n",
    "    for face in faces:\n",
    "        cropped_face = face['box']  # This gives you the bounding box of the face\n",
    "        \n",
    "        gray = rgb2gray(raw_img)\n",
    "        gray = resize(gray, (48, 48), mode='symmetric').astype(np.uint8)\n",
    "    \n",
    "        img = gray[:, :, np.newaxis]\n",
    "        img = np.concatenate((img, img, img), axis=2)\n",
    "        img = Image.fromarray(img)\n",
    "        inputs = transform_test(img)\n",
    "    \n",
    "        ncrops, c, h, w = np.shape(inputs)\n",
    "        inputs = inputs.view(-1, c, h, w)\n",
    "        inputs = inputs.cuda()\n",
    "    \n",
    "        # Note: 'volatile' is deprecated. Instead, use 'with torch.no_grad():' for inference\n",
    "        with torch.no_grad():\n",
    "            embed = transfer_net(inputs)\n",
    "        \n",
    "        # Check validity of embedding\n",
    "        if torch.any(embed):  # If any value in the embedding tensor is non-zero\n",
    "            img_embeddings.append(embed.cpu().numpy())\n",
    "\n",
    "        \n",
    "    # Average the embeddings for this image\n",
    "    if img_embeddings:  # If there's at least one valid embedding\n",
    "        avg_embed = np.mean(img_embeddings, axis=0)\n",
    "    else:\n",
    "        avg_embed = np.zeros((256,))  # Placeholder for no embedding\n",
    "    \n",
    "    # Add averaged embedding and related info to lists\n",
    "    averaged_embeddings_first_half.append(avg_embed)\n",
    "    base_name = os.path.splitext(img_name)[0]\n",
    "    image_names_first_half.append(base_name)\n",
    "\n",
    "\n",
    "# Create a DataFrame with averaged embeddings and related info for the first half\n",
    "df_avg_embeddings_first_half = pd.DataFrame({\n",
    "    'ImageName': image_names_first_half,\n",
    "    'Embedding': averaged_embeddings_first_half\n",
    "})\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Averaging each list of embeddings\n",
    "averaged_embeddings = [np.mean(embedding, axis=0) for embedding in df_avg_embeddings_first_half['Embedding']]\n",
    "\n",
    "# Filter the rows where the 'Embedding' column is not an instance of numpy.ndarray\n",
    "df_cleaned = df_avg_embeddings_first_half[df_avg_embeddings_first_half['Embedding'].apply(lambda x: isinstance(x, np.ndarray))]\n",
    "\n",
    "# If you want to reset the index after dropping:\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert the 'Embedding' column from a nested list to a numpy array\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(np.array)\n",
    "\n",
    "# Average the embeddings along axis=1\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(lambda x: x.mean(axis=0) if len(x.shape) > 1 else x)\n",
    "\n",
    "# Expand the averaged embeddings into individual columns\n",
    "embeddings_df = df_cleaned['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "embeddings_df.columns = [f'embed_{i}' for i in range(embeddings_df.shape[1])]\n",
    "\n",
    "# Drop the original 'Embedding' column and concatenate the expanded columns\n",
    "df_avg_embeddings_first_half = pd.concat([df_cleaned.drop('Embedding', axis=1), embeddings_df], axis=1)\n",
    "\n",
    "\n",
    "df_avg_embeddings_first_half.to_csv('645_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1a14c3",
   "metadata": {},
   "source": [
    "# Merging all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c921adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_1 = pd.read_csv('645_1.csv')\n",
    "df_2 = pd.read_csv('645_2.csv')\n",
    "df_3 = pd.read_csv('645_3.csv')\n",
    "df_4 = pd.read_csv('645_4.csv')\n",
    "df_5 = pd.read_csv('645_5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "18dc8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [df_1, df_2, df_3, df_4, df_5]\n",
    "\n",
    "# Vertically stack the dataframes\n",
    "df_combined = pd.concat(dfs, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6238791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SCENE</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1_10004</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_10009</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1001</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_1003</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1_10190</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Sarcasm\n",
       "SCENE           \n",
       "1_10004      0.0\n",
       "1_10009      0.0\n",
       "1_1001       0.0\n",
       "1_1003       1.0\n",
       "1_10190      0.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('labels_final.csv', index_col='SCENE')\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10c8104",
   "metadata": {},
   "source": [
    "# Merging with Label\n",
    "This is for ease of use. The data can be loaded and split into train and test easily including labels without reading and cleaning  again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cd99b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remove the suffix from 'ImageName'\n",
    "df['ImageName'] = df['ImageName'].str.split('_u_').str[0]\n",
    "\n",
    "# Merge the two DataFrames based on 'ImageName' and 'SCENE'\n",
    "merged_df = df.merge(labels, left_on='ImageName', right_on='SCENE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27cd0fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_247</th>\n",
       "      <th>embed_248</th>\n",
       "      <th>embed_249</th>\n",
       "      <th>embed_250</th>\n",
       "      <th>embed_251</th>\n",
       "      <th>embed_252</th>\n",
       "      <th>embed_253</th>\n",
       "      <th>embed_254</th>\n",
       "      <th>embed_255</th>\n",
       "      <th>Sarcasm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_10004</td>\n",
       "      <td>0.257854</td>\n",
       "      <td>0.066377</td>\n",
       "      <td>0.161984</td>\n",
       "      <td>0.105779</td>\n",
       "      <td>0.237214</td>\n",
       "      <td>0.453038</td>\n",
       "      <td>-0.026357</td>\n",
       "      <td>0.004962</td>\n",
       "      <td>-0.227729</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046718</td>\n",
       "      <td>-0.101532</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.237791</td>\n",
       "      <td>0.022939</td>\n",
       "      <td>-0.099684</td>\n",
       "      <td>0.293339</td>\n",
       "      <td>0.418022</td>\n",
       "      <td>0.179787</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_10004</td>\n",
       "      <td>0.250770</td>\n",
       "      <td>0.088195</td>\n",
       "      <td>0.178239</td>\n",
       "      <td>0.137102</td>\n",
       "      <td>0.245249</td>\n",
       "      <td>0.393889</td>\n",
       "      <td>-0.063021</td>\n",
       "      <td>0.039276</td>\n",
       "      <td>-0.238393</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049494</td>\n",
       "      <td>-0.143644</td>\n",
       "      <td>-0.037926</td>\n",
       "      <td>0.217017</td>\n",
       "      <td>0.007791</td>\n",
       "      <td>-0.135646</td>\n",
       "      <td>0.274866</td>\n",
       "      <td>0.379432</td>\n",
       "      <td>0.159771</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_10004</td>\n",
       "      <td>0.245529</td>\n",
       "      <td>0.054224</td>\n",
       "      <td>0.139849</td>\n",
       "      <td>0.151603</td>\n",
       "      <td>0.246439</td>\n",
       "      <td>0.417611</td>\n",
       "      <td>-0.054625</td>\n",
       "      <td>0.056534</td>\n",
       "      <td>-0.256085</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.067398</td>\n",
       "      <td>-0.164051</td>\n",
       "      <td>-0.026499</td>\n",
       "      <td>0.230718</td>\n",
       "      <td>-0.007253</td>\n",
       "      <td>-0.108036</td>\n",
       "      <td>0.289110</td>\n",
       "      <td>0.395327</td>\n",
       "      <td>0.169015</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_10004</td>\n",
       "      <td>0.297613</td>\n",
       "      <td>0.131141</td>\n",
       "      <td>0.179681</td>\n",
       "      <td>0.152630</td>\n",
       "      <td>0.248578</td>\n",
       "      <td>0.505152</td>\n",
       "      <td>-0.043195</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>-0.234365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.091981</td>\n",
       "      <td>-0.148397</td>\n",
       "      <td>-0.031532</td>\n",
       "      <td>0.207091</td>\n",
       "      <td>0.041971</td>\n",
       "      <td>-0.116478</td>\n",
       "      <td>0.295764</td>\n",
       "      <td>0.486895</td>\n",
       "      <td>0.189584</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_10009</td>\n",
       "      <td>0.290877</td>\n",
       "      <td>0.090882</td>\n",
       "      <td>0.200734</td>\n",
       "      <td>-0.031469</td>\n",
       "      <td>0.171222</td>\n",
       "      <td>0.474413</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>-0.071997</td>\n",
       "      <td>-0.168710</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002605</td>\n",
       "      <td>0.031383</td>\n",
       "      <td>0.059292</td>\n",
       "      <td>0.231099</td>\n",
       "      <td>0.114619</td>\n",
       "      <td>-0.136372</td>\n",
       "      <td>0.261263</td>\n",
       "      <td>0.390234</td>\n",
       "      <td>0.186231</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>3_S06E05_355</td>\n",
       "      <td>-0.167053</td>\n",
       "      <td>0.115846</td>\n",
       "      <td>-0.262147</td>\n",
       "      <td>0.410062</td>\n",
       "      <td>0.134311</td>\n",
       "      <td>0.074633</td>\n",
       "      <td>-0.334752</td>\n",
       "      <td>0.083559</td>\n",
       "      <td>0.059221</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147792</td>\n",
       "      <td>0.084459</td>\n",
       "      <td>-0.072884</td>\n",
       "      <td>0.174486</td>\n",
       "      <td>0.033017</td>\n",
       "      <td>-0.238315</td>\n",
       "      <td>-0.221664</td>\n",
       "      <td>-0.186336</td>\n",
       "      <td>-0.055229</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3038</th>\n",
       "      <td>3_S06E05_355</td>\n",
       "      <td>-0.172933</td>\n",
       "      <td>0.141143</td>\n",
       "      <td>-0.168410</td>\n",
       "      <td>0.389916</td>\n",
       "      <td>0.139194</td>\n",
       "      <td>0.036480</td>\n",
       "      <td>-0.321985</td>\n",
       "      <td>0.083749</td>\n",
       "      <td>0.025253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.132260</td>\n",
       "      <td>0.097110</td>\n",
       "      <td>-0.053377</td>\n",
       "      <td>0.101862</td>\n",
       "      <td>0.008746</td>\n",
       "      <td>-0.181637</td>\n",
       "      <td>-0.218261</td>\n",
       "      <td>-0.144200</td>\n",
       "      <td>-0.000148</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3039</th>\n",
       "      <td>3_S06E06_143</td>\n",
       "      <td>-0.111265</td>\n",
       "      <td>-0.082199</td>\n",
       "      <td>-0.102560</td>\n",
       "      <td>0.181393</td>\n",
       "      <td>0.017090</td>\n",
       "      <td>0.045054</td>\n",
       "      <td>-0.135264</td>\n",
       "      <td>0.047553</td>\n",
       "      <td>-0.027247</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073729</td>\n",
       "      <td>0.047142</td>\n",
       "      <td>-0.009305</td>\n",
       "      <td>0.036020</td>\n",
       "      <td>0.081214</td>\n",
       "      <td>-0.242498</td>\n",
       "      <td>-0.238130</td>\n",
       "      <td>-0.076463</td>\n",
       "      <td>-0.202183</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3040</th>\n",
       "      <td>3_S06E06_143</td>\n",
       "      <td>-0.191432</td>\n",
       "      <td>0.055875</td>\n",
       "      <td>-0.214800</td>\n",
       "      <td>0.344588</td>\n",
       "      <td>0.159469</td>\n",
       "      <td>0.064737</td>\n",
       "      <td>-0.296399</td>\n",
       "      <td>0.076148</td>\n",
       "      <td>0.057796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153785</td>\n",
       "      <td>0.042271</td>\n",
       "      <td>-0.075402</td>\n",
       "      <td>0.187334</td>\n",
       "      <td>0.047505</td>\n",
       "      <td>-0.246841</td>\n",
       "      <td>-0.205726</td>\n",
       "      <td>-0.112969</td>\n",
       "      <td>-0.038097</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3041</th>\n",
       "      <td>3_S06E07_272</td>\n",
       "      <td>-0.137637</td>\n",
       "      <td>-0.043770</td>\n",
       "      <td>-0.333955</td>\n",
       "      <td>0.308425</td>\n",
       "      <td>0.111118</td>\n",
       "      <td>0.144482</td>\n",
       "      <td>-0.254176</td>\n",
       "      <td>0.064772</td>\n",
       "      <td>0.116151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.156087</td>\n",
       "      <td>0.055915</td>\n",
       "      <td>-0.088088</td>\n",
       "      <td>0.285233</td>\n",
       "      <td>0.106566</td>\n",
       "      <td>-0.371206</td>\n",
       "      <td>-0.188087</td>\n",
       "      <td>-0.228178</td>\n",
       "      <td>-0.228764</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3042 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageName   embed_0   embed_1   embed_2   embed_3   embed_4  \\\n",
       "0          1_10004  0.257854  0.066377  0.161984  0.105779  0.237214   \n",
       "1          1_10004  0.250770  0.088195  0.178239  0.137102  0.245249   \n",
       "2          1_10004  0.245529  0.054224  0.139849  0.151603  0.246439   \n",
       "3          1_10004  0.297613  0.131141  0.179681  0.152630  0.248578   \n",
       "4          1_10009  0.290877  0.090882  0.200734 -0.031469  0.171222   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "3037  3_S06E05_355 -0.167053  0.115846 -0.262147  0.410062  0.134311   \n",
       "3038  3_S06E05_355 -0.172933  0.141143 -0.168410  0.389916  0.139194   \n",
       "3039  3_S06E06_143 -0.111265 -0.082199 -0.102560  0.181393  0.017090   \n",
       "3040  3_S06E06_143 -0.191432  0.055875 -0.214800  0.344588  0.159469   \n",
       "3041  3_S06E07_272 -0.137637 -0.043770 -0.333955  0.308425  0.111118   \n",
       "\n",
       "       embed_5   embed_6   embed_7   embed_8  ...  embed_247  embed_248  \\\n",
       "0     0.453038 -0.026357  0.004962 -0.227729  ...  -0.046718  -0.101532   \n",
       "1     0.393889 -0.063021  0.039276 -0.238393  ...  -0.049494  -0.143644   \n",
       "2     0.417611 -0.054625  0.056534 -0.256085  ...  -0.067398  -0.164051   \n",
       "3     0.505152 -0.043195  0.001284 -0.234365  ...  -0.091981  -0.148397   \n",
       "4     0.474413  0.004307 -0.071997 -0.168710  ...  -0.002605   0.031383   \n",
       "...        ...       ...       ...       ...  ...        ...        ...   \n",
       "3037  0.074633 -0.334752  0.083559  0.059221  ...   0.147792   0.084459   \n",
       "3038  0.036480 -0.321985  0.083749  0.025253  ...   0.132260   0.097110   \n",
       "3039  0.045054 -0.135264  0.047553 -0.027247  ...   0.073729   0.047142   \n",
       "3040  0.064737 -0.296399  0.076148  0.057796  ...   0.153785   0.042271   \n",
       "3041  0.144482 -0.254176  0.064772  0.116151  ...   0.156087   0.055915   \n",
       "\n",
       "      embed_249  embed_250  embed_251  embed_252  embed_253  embed_254  \\\n",
       "0      0.000772   0.237791   0.022939  -0.099684   0.293339   0.418022   \n",
       "1     -0.037926   0.217017   0.007791  -0.135646   0.274866   0.379432   \n",
       "2     -0.026499   0.230718  -0.007253  -0.108036   0.289110   0.395327   \n",
       "3     -0.031532   0.207091   0.041971  -0.116478   0.295764   0.486895   \n",
       "4      0.059292   0.231099   0.114619  -0.136372   0.261263   0.390234   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "3037  -0.072884   0.174486   0.033017  -0.238315  -0.221664  -0.186336   \n",
       "3038  -0.053377   0.101862   0.008746  -0.181637  -0.218261  -0.144200   \n",
       "3039  -0.009305   0.036020   0.081214  -0.242498  -0.238130  -0.076463   \n",
       "3040  -0.075402   0.187334   0.047505  -0.246841  -0.205726  -0.112969   \n",
       "3041  -0.088088   0.285233   0.106566  -0.371206  -0.188087  -0.228178   \n",
       "\n",
       "      embed_255  Sarcasm  \n",
       "0      0.179787      0.0  \n",
       "1      0.159771      0.0  \n",
       "2      0.169015      0.0  \n",
       "3      0.189584      0.0  \n",
       "4      0.186231      0.0  \n",
       "...         ...      ...  \n",
       "3037  -0.055229      1.0  \n",
       "3038  -0.000148      1.0  \n",
       "3039  -0.202183      1.0  \n",
       "3040  -0.038097      1.0  \n",
       "3041  -0.228764      1.0  \n",
       "\n",
       "[3042 rows x 258 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa716364",
   "metadata": {},
   "source": [
    "# Saving dataframe with embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e4ebc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv('final_image_embeddings.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
