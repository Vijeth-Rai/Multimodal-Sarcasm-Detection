{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b42c6f3",
   "metadata": {},
   "source": [
    "# Facial Expression Recognition Model\n",
    "\n",
    "This model and weights are from https://github.com/WuJie1010/Facial-Expression-Recognition.Pytorch The github project is cloned and modified to fit the required needs.\n",
    "\n",
    "The images in localized folder, which contains only faces of the sarcastic speaker through manual cropping. All the images in this folder are sent through the model to generate embeddings. These embeddings are expected to capture the important features of the facial expression of the speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4268bbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1203 [00:00<?, ?it/s]C:\\Users\\vijet\\Projects\\MSc\\mustard++\\Facial-Expression-Recognition.Pytorch-master\\transforms\\functional.py:63: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  img = torch.ByteTensor(torch.ByteStorage.from_buffer(pic.tobytes()))\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 1203/1203 [00:09<00:00, 123.56it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import transforms as transforms\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "from models import *\n",
    "import pandas as pd\n",
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "mtcnn = MTCNN()\n",
    "\n",
    "cut_size = 44\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.TenCrop(cut_size),\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "])\n",
    "\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.299, 0.587, 0.114])\n",
    "\n",
    "class ModifiedVGG(nn.Module):\n",
    "    def __init__(self, vgg_name, embedding_dim=256):\n",
    "        super(ModifiedVGG, self).__init__()\n",
    "        \n",
    "        # Original VGG features\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        \n",
    "        # Embedding layer to get the embeddings from the model\n",
    "        self.embedding_layer = nn.Linear(512, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        # Pass through the embedding layer\n",
    "        embedding = self.embedding_layer(out)\n",
    "        \n",
    "        return embedding\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "# Load pre-trained weights into modified VGG\n",
    "transfer_net = ModifiedVGG('VGG19')\n",
    "checkpoint = torch.load(os.path.join('FER2013_VGG19', 'PrivateTest_model.t7'))\n",
    "\n",
    "# Remove the 'classifier' weights from the checkpoint as they don't exist in the ModifiedVGG\n",
    "checkpoint['net'] = {k: v for k, v in checkpoint['net'].items() if 'classifier' not in k}\n",
    "transfer_net.load_state_dict(checkpoint['net'], strict=False)  \n",
    "\n",
    "transfer_net.cuda()\n",
    "transfer_net.eval()\n",
    "\n",
    "# Prepare to collect embeddings and filenames\n",
    "averaged_embeddings_first_half = []\n",
    "image_names_first_half = []\n",
    "\n",
    "image_names = os.listdir('localized/')\n",
    "\n",
    "for img_name in tqdm(image_names):\n",
    "    img_path = os.path.join('localized/', img_name)\n",
    "    \n",
    "    raw_img = io.imread(img_path)\n",
    "    \n",
    "    img_embeddings = []  # To collect embeddings for this image\n",
    "    \n",
    "    gray = rgb2gray(raw_img)\n",
    "    gray = resize(gray, (48, 48), mode='symmetric').astype(np.uint8)\n",
    "    \n",
    "    img = gray[:, :, np.newaxis]\n",
    "    img = np.concatenate((img, img, img), axis=2)\n",
    "    img = Image.fromarray(img)\n",
    "    inputs = transform_test(img)\n",
    "    \n",
    "    ncrops, c, h, w = np.shape(inputs)\n",
    "    inputs = inputs.view(-1, c, h, w)\n",
    "    inputs = inputs.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embed = transfer_net(inputs)\n",
    "        \n",
    "    # Check validity of embedding\n",
    "    if torch.any(embed):  # If any value in the embedding tensor is non-zero\n",
    "        embed_avg = embed.view(ncrops, -1).mean(0)\n",
    "        img_embeddings.append(embed_avg.cpu().numpy())\n",
    "            \n",
    "        \n",
    "    # Average the embeddings for this image\n",
    "    if img_embeddings:  # If there's at least one valid embedding\n",
    "        avg_embed = np.mean(img_embeddings, axis=0)\n",
    "    else:\n",
    "        avg_embed = np.zeros((256,))  # Placeholder for no embedding\n",
    "    \n",
    "    # Add averaged embedding and related info to lists\n",
    "    averaged_embeddings_first_half.append(avg_embed)\n",
    "    base_name = os.path.splitext(img_name)[0]\n",
    "    image_names_first_half.append(base_name)\n",
    "\n",
    "\n",
    "# Create a DataFrame with averaged embeddings and related info for the first half\n",
    "df_avg_embeddings_first_half = pd.DataFrame({\n",
    "    'ImageName': image_names_first_half,\n",
    "    'Embedding': averaged_embeddings_first_half\n",
    "})\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Averaging each list of embeddings\n",
    "averaged_embeddings = [np.mean(embedding, axis=0) for embedding in df_avg_embeddings_first_half['Embedding']]\n",
    "\n",
    "# Filter the rows where the 'Embedding' column is not an instance of numpy.ndarray\n",
    "df_cleaned = df_avg_embeddings_first_half[df_avg_embeddings_first_half['Embedding'].apply(lambda x: isinstance(x, np.ndarray))]\n",
    "\n",
    "# If you want to reset the index after dropping:\n",
    "df_cleaned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Convert the 'Embedding' column from a nested list to a numpy array\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(np.array)\n",
    "\n",
    "# Average the embeddings along axis=1\n",
    "df_cleaned['Embedding'] = df_cleaned['Embedding'].apply(lambda x: x.mean(axis=0) if len(x.shape) > 1 else x)\n",
    "\n",
    "# Expand the averaged embeddings into individual columns\n",
    "embeddings_df = df_cleaned['Embedding'].apply(pd.Series)\n",
    "\n",
    "# Rename columns\n",
    "embeddings_df.columns = [f'embed_{i}' for i in range(embeddings_df.shape[1])]\n",
    "\n",
    "# Drop the original 'Embedding' column and concatenate the expanded columns\n",
    "df_avg_embeddings_first_half = pd.concat([df_cleaned.drop('Embedding', axis=1), embeddings_df], axis=1)\n",
    "\n",
    "\n",
    "df_avg_embeddings_first_half.to_csv('dependant_image_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12c33c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageName</th>\n",
       "      <th>embed_0</th>\n",
       "      <th>embed_1</th>\n",
       "      <th>embed_2</th>\n",
       "      <th>embed_3</th>\n",
       "      <th>embed_4</th>\n",
       "      <th>embed_5</th>\n",
       "      <th>embed_6</th>\n",
       "      <th>embed_7</th>\n",
       "      <th>embed_8</th>\n",
       "      <th>...</th>\n",
       "      <th>embed_246</th>\n",
       "      <th>embed_247</th>\n",
       "      <th>embed_248</th>\n",
       "      <th>embed_249</th>\n",
       "      <th>embed_250</th>\n",
       "      <th>embed_251</th>\n",
       "      <th>embed_252</th>\n",
       "      <th>embed_253</th>\n",
       "      <th>embed_254</th>\n",
       "      <th>embed_255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1_10004_u</td>\n",
       "      <td>0.225880</td>\n",
       "      <td>-0.158256</td>\n",
       "      <td>-0.166669</td>\n",
       "      <td>0.061559</td>\n",
       "      <td>0.061490</td>\n",
       "      <td>-0.127280</td>\n",
       "      <td>0.202243</td>\n",
       "      <td>-0.420007</td>\n",
       "      <td>0.017750</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043996</td>\n",
       "      <td>0.237813</td>\n",
       "      <td>0.044273</td>\n",
       "      <td>-0.251303</td>\n",
       "      <td>0.176454</td>\n",
       "      <td>-0.067825</td>\n",
       "      <td>-0.255763</td>\n",
       "      <td>0.014547</td>\n",
       "      <td>0.411552</td>\n",
       "      <td>0.008522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1_10009_u</td>\n",
       "      <td>0.083695</td>\n",
       "      <td>-0.134326</td>\n",
       "      <td>-0.245697</td>\n",
       "      <td>0.065822</td>\n",
       "      <td>-0.059156</td>\n",
       "      <td>0.034589</td>\n",
       "      <td>-0.001629</td>\n",
       "      <td>0.015762</td>\n",
       "      <td>-0.117956</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.158322</td>\n",
       "      <td>-0.008103</td>\n",
       "      <td>0.089971</td>\n",
       "      <td>0.054675</td>\n",
       "      <td>0.185983</td>\n",
       "      <td>-0.148866</td>\n",
       "      <td>-0.155370</td>\n",
       "      <td>0.067999</td>\n",
       "      <td>0.367675</td>\n",
       "      <td>0.082560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1_1001_u</td>\n",
       "      <td>0.105431</td>\n",
       "      <td>-0.061188</td>\n",
       "      <td>-0.176715</td>\n",
       "      <td>0.122682</td>\n",
       "      <td>0.072980</td>\n",
       "      <td>-0.042338</td>\n",
       "      <td>0.119878</td>\n",
       "      <td>-0.118731</td>\n",
       "      <td>-0.175453</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121635</td>\n",
       "      <td>0.160505</td>\n",
       "      <td>0.091331</td>\n",
       "      <td>-0.093659</td>\n",
       "      <td>0.209373</td>\n",
       "      <td>0.014874</td>\n",
       "      <td>-0.181255</td>\n",
       "      <td>0.152046</td>\n",
       "      <td>0.253700</td>\n",
       "      <td>-0.038468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1_1003_u</td>\n",
       "      <td>0.118452</td>\n",
       "      <td>-0.078877</td>\n",
       "      <td>-0.113624</td>\n",
       "      <td>0.062875</td>\n",
       "      <td>0.110320</td>\n",
       "      <td>-0.154886</td>\n",
       "      <td>0.224045</td>\n",
       "      <td>-0.334346</td>\n",
       "      <td>-0.051198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.183090</td>\n",
       "      <td>0.205164</td>\n",
       "      <td>0.064420</td>\n",
       "      <td>-0.214857</td>\n",
       "      <td>0.174033</td>\n",
       "      <td>-0.063176</td>\n",
       "      <td>-0.213527</td>\n",
       "      <td>0.078609</td>\n",
       "      <td>0.255371</td>\n",
       "      <td>-0.067467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1_10190_u</td>\n",
       "      <td>0.033148</td>\n",
       "      <td>-0.215622</td>\n",
       "      <td>-0.285165</td>\n",
       "      <td>0.128291</td>\n",
       "      <td>0.151158</td>\n",
       "      <td>-0.038314</td>\n",
       "      <td>0.059698</td>\n",
       "      <td>-0.374993</td>\n",
       "      <td>-0.019645</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042739</td>\n",
       "      <td>0.176863</td>\n",
       "      <td>0.065676</td>\n",
       "      <td>-0.181788</td>\n",
       "      <td>0.148470</td>\n",
       "      <td>-0.120253</td>\n",
       "      <td>-0.389759</td>\n",
       "      <td>-0.141423</td>\n",
       "      <td>0.539212</td>\n",
       "      <td>0.048756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1198</th>\n",
       "      <td>3_S06E03_366_u</td>\n",
       "      <td>0.258991</td>\n",
       "      <td>-0.254466</td>\n",
       "      <td>-0.103983</td>\n",
       "      <td>-0.041723</td>\n",
       "      <td>-0.045361</td>\n",
       "      <td>-0.180650</td>\n",
       "      <td>0.273379</td>\n",
       "      <td>-0.505350</td>\n",
       "      <td>0.107875</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031126</td>\n",
       "      <td>0.243304</td>\n",
       "      <td>-0.031462</td>\n",
       "      <td>-0.103157</td>\n",
       "      <td>0.131190</td>\n",
       "      <td>-0.101826</td>\n",
       "      <td>-0.130989</td>\n",
       "      <td>-0.011995</td>\n",
       "      <td>0.393863</td>\n",
       "      <td>0.026977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1199</th>\n",
       "      <td>3_S06E05_355_u</td>\n",
       "      <td>0.332637</td>\n",
       "      <td>-0.135706</td>\n",
       "      <td>-0.076136</td>\n",
       "      <td>0.022215</td>\n",
       "      <td>0.003767</td>\n",
       "      <td>-0.159307</td>\n",
       "      <td>0.278680</td>\n",
       "      <td>-0.401238</td>\n",
       "      <td>-0.023301</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068622</td>\n",
       "      <td>0.234057</td>\n",
       "      <td>0.057899</td>\n",
       "      <td>-0.219143</td>\n",
       "      <td>0.169299</td>\n",
       "      <td>-0.072238</td>\n",
       "      <td>-0.128961</td>\n",
       "      <td>0.131588</td>\n",
       "      <td>0.275824</td>\n",
       "      <td>0.022364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>3_S06E06_143_u</td>\n",
       "      <td>-0.017695</td>\n",
       "      <td>-0.118993</td>\n",
       "      <td>-0.109349</td>\n",
       "      <td>0.032382</td>\n",
       "      <td>0.041774</td>\n",
       "      <td>-0.024940</td>\n",
       "      <td>0.048582</td>\n",
       "      <td>-0.222080</td>\n",
       "      <td>-0.071703</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134897</td>\n",
       "      <td>0.062996</td>\n",
       "      <td>0.060295</td>\n",
       "      <td>-0.037984</td>\n",
       "      <td>0.183025</td>\n",
       "      <td>-0.163765</td>\n",
       "      <td>-0.172335</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.237152</td>\n",
       "      <td>-0.039465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>3_S06E07_272_u</td>\n",
       "      <td>0.107657</td>\n",
       "      <td>-0.187309</td>\n",
       "      <td>-0.065122</td>\n",
       "      <td>0.017533</td>\n",
       "      <td>0.048842</td>\n",
       "      <td>-0.065346</td>\n",
       "      <td>0.108859</td>\n",
       "      <td>-0.266657</td>\n",
       "      <td>-0.188629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196729</td>\n",
       "      <td>0.030840</td>\n",
       "      <td>0.067684</td>\n",
       "      <td>-0.122658</td>\n",
       "      <td>0.105194</td>\n",
       "      <td>-0.068256</td>\n",
       "      <td>-0.036879</td>\n",
       "      <td>0.113403</td>\n",
       "      <td>0.028594</td>\n",
       "      <td>-0.002684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>Copy of 1_105_u</td>\n",
       "      <td>0.053780</td>\n",
       "      <td>-0.173355</td>\n",
       "      <td>-0.274670</td>\n",
       "      <td>0.170405</td>\n",
       "      <td>0.009889</td>\n",
       "      <td>-0.066143</td>\n",
       "      <td>0.016296</td>\n",
       "      <td>-0.122462</td>\n",
       "      <td>-0.151250</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202748</td>\n",
       "      <td>-0.038938</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>-0.066471</td>\n",
       "      <td>0.187240</td>\n",
       "      <td>-0.159801</td>\n",
       "      <td>-0.155950</td>\n",
       "      <td>-0.005307</td>\n",
       "      <td>0.360331</td>\n",
       "      <td>0.172055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1203 rows × 257 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ImageName   embed_0   embed_1   embed_2   embed_3   embed_4  \\\n",
       "0           1_10004_u  0.225880 -0.158256 -0.166669  0.061559  0.061490   \n",
       "1           1_10009_u  0.083695 -0.134326 -0.245697  0.065822 -0.059156   \n",
       "2            1_1001_u  0.105431 -0.061188 -0.176715  0.122682  0.072980   \n",
       "3            1_1003_u  0.118452 -0.078877 -0.113624  0.062875  0.110320   \n",
       "4           1_10190_u  0.033148 -0.215622 -0.285165  0.128291  0.151158   \n",
       "...               ...       ...       ...       ...       ...       ...   \n",
       "1198   3_S06E03_366_u  0.258991 -0.254466 -0.103983 -0.041723 -0.045361   \n",
       "1199   3_S06E05_355_u  0.332637 -0.135706 -0.076136  0.022215  0.003767   \n",
       "1200   3_S06E06_143_u -0.017695 -0.118993 -0.109349  0.032382  0.041774   \n",
       "1201   3_S06E07_272_u  0.107657 -0.187309 -0.065122  0.017533  0.048842   \n",
       "1202  Copy of 1_105_u  0.053780 -0.173355 -0.274670  0.170405  0.009889   \n",
       "\n",
       "       embed_5   embed_6   embed_7   embed_8  ...  embed_246  embed_247  \\\n",
       "0    -0.127280  0.202243 -0.420007  0.017750  ...   0.043996   0.237813   \n",
       "1     0.034589 -0.001629  0.015762 -0.117956  ...  -0.158322  -0.008103   \n",
       "2    -0.042338  0.119878 -0.118731 -0.175453  ...   0.121635   0.160505   \n",
       "3    -0.154886  0.224045 -0.334346 -0.051198  ...   0.183090   0.205164   \n",
       "4    -0.038314  0.059698 -0.374993 -0.019645  ...  -0.042739   0.176863   \n",
       "...        ...       ...       ...       ...  ...        ...        ...   \n",
       "1198 -0.180650  0.273379 -0.505350  0.107875  ...  -0.031126   0.243304   \n",
       "1199 -0.159307  0.278680 -0.401238 -0.023301  ...   0.068622   0.234057   \n",
       "1200 -0.024940  0.048582 -0.222080 -0.071703  ...   0.134897   0.062996   \n",
       "1201 -0.065346  0.108859 -0.266657 -0.188629  ...   0.196729   0.030840   \n",
       "1202 -0.066143  0.016296 -0.122462 -0.151250  ...  -0.202748  -0.038938   \n",
       "\n",
       "      embed_248  embed_249  embed_250  embed_251  embed_252  embed_253  \\\n",
       "0      0.044273  -0.251303   0.176454  -0.067825  -0.255763   0.014547   \n",
       "1      0.089971   0.054675   0.185983  -0.148866  -0.155370   0.067999   \n",
       "2      0.091331  -0.093659   0.209373   0.014874  -0.181255   0.152046   \n",
       "3      0.064420  -0.214857   0.174033  -0.063176  -0.213527   0.078609   \n",
       "4      0.065676  -0.181788   0.148470  -0.120253  -0.389759  -0.141423   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "1198  -0.031462  -0.103157   0.131190  -0.101826  -0.130989  -0.011995   \n",
       "1199   0.057899  -0.219143   0.169299  -0.072238  -0.128961   0.131588   \n",
       "1200   0.060295  -0.037984   0.183025  -0.163765  -0.172335   0.008819   \n",
       "1201   0.067684  -0.122658   0.105194  -0.068256  -0.036879   0.113403   \n",
       "1202   0.088542  -0.066471   0.187240  -0.159801  -0.155950  -0.005307   \n",
       "\n",
       "      embed_254  embed_255  \n",
       "0      0.411552   0.008522  \n",
       "1      0.367675   0.082560  \n",
       "2      0.253700  -0.038468  \n",
       "3      0.255371  -0.067467  \n",
       "4      0.539212   0.048756  \n",
       "...         ...        ...  \n",
       "1198   0.393863   0.026977  \n",
       "1199   0.275824   0.022364  \n",
       "1200   0.237152  -0.039465  \n",
       "1201   0.028594  -0.002684  \n",
       "1202   0.360331   0.172055  \n",
       "\n",
       "[1203 rows x 257 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_avg_embeddings_first_half"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10d6f8c",
   "metadata": {},
   "source": [
    "the dataframe is only named df_avg_embeddings_first_half, it contains the entire data. The naming is such because this notebook was created after Independant Image embedding notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
